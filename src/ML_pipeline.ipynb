{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script trains and select models for this problem\n",
    "# load the packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed_sleep_fft.csv')\n",
    "cat_ftrs = ['channel_name']\n",
    "scalar_ftrs = ['alpha', 'theta', 'slowwave', 'sigma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balance 0 0.1071059175634339\n",
      "balance 1 0.1365286855482934\n",
      "balance 2 0.09823727470786295\n",
      "balance 3 0.34462269756387404\n",
      "balance 4 0.31350542461653574\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['label'])\n",
    "subject_ID = df['subject']\n",
    "nap_ID = df['NAP']\n",
    "dropc = ['Unnamed: 0', 'label', 'subject', 'NAP']\n",
    "X = df.drop(columns= dropc)\n",
    "# check balance \n",
    "classes, counts = np.unique(y, return_counts=True)\n",
    "for i in range(len(classes)):\n",
    "    print ('balance', i, counts[i]/ len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode groups 4 subject ID * 2 naps \n",
    "import itertools\n",
    "n_subject = np.unique(subject_ID)\n",
    "n_nap = np.unique(nap_ID)\n",
    "iterset = (list(itertools.product(n_subject, n_nap)))\n",
    "group = np.zeros((len(subject_ID), 1))\n",
    "i = 0\n",
    "for sbj, nap in iterset:\n",
    "    idx = np.logical_and(subject_ID == sbj, nap_ID== nap)\n",
    "    group[idx] = i\n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GroupKFold\n",
    "# from sklearn.model_selection import GroupShuffleSplit\n",
    "# def ML_pipeline_groups_GridSearchCV_SVC(X,y,groups,random_state,n_folds):\n",
    "#     # create a test set based on groups\n",
    "#     splitter = GroupShuffleSplit(n_splits=1,test_size=0.2,random_state=random_state)\n",
    "#     for i_other,i_test in splitter.split(X, y, groups):\n",
    "#         X_other, y_other, groups_other = X.iloc[i_other], y[i_other], groups[i_other]\n",
    "#         X_test, y_test, groups_test = X.iloc[i_test], y[i_test], groups[i_test]\n",
    "#     # splitter for _other\n",
    "#     kf = GroupKFold(n_splits=n_folds)\n",
    "#     # create the pipeline: preprocessor + supervised ML method\n",
    "#     cat_ftrs = ['channel_name']\n",
    "#     cont_ftrs = ['alpha', 'theta', 'slowwave', 'sigma']\n",
    "#     cat_transformer = Pipeline(steps = [\n",
    "#         ('imputer1', SimpleImputer(missing_values='0.0', strategy='constant',fill_value='missing')),\n",
    "#         ('onehot', OneHotEncoder(sparse = False, categories = 'auto'))])\n",
    "#     cont_transformer = Pipeline(steps = [\n",
    "#         ('imputer2', SimpleImputer(missing_values = np.nan,strategy = 'mean')),\n",
    "#         ('scaler', StandardScaler())])\n",
    "#     preprocessor = ColumnTransformer(remainder='passthrough',\n",
    "#     transformers=[\n",
    "#         ('num', cont_transformer, cont_ftrs),\n",
    "#         ('cat', cat_transformer, cat_ftrs)])\n",
    "    \n",
    "#     # make overall pipeline\n",
    "#     pipe = make_pipeline(\n",
    "#         preprocessor,\n",
    "#         SVC(probability = True, max_iter = 1000))\n",
    "\n",
    "#     # the parameter(s) we want to tune\n",
    "#     param_grid = {'svc__C': np.logspace(-2,2,num=5),'svc__gamma': np.logspace(-2,2,num=5)}\n",
    "#     # prepare gridsearch\n",
    "#     grid = GridSearchCV(pipe, param_grid=param_grid,scoring = make_scorer(accuracy_score),\n",
    "#                         cv=kf, return_train_score = True,iid=True, n_jobs = -1)\n",
    "#     # do kfold CV on _other\n",
    "#     grid.fit(X_other, y_other, groups_other)\n",
    "#     return grid, grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 0 process\n"
     ]
    }
   ],
   "source": [
    "# test_scores_SVC = []\n",
    "# for i in range(5):\n",
    "#     print('Starting:', i, 'process')\n",
    "#     grid, test_score = ML_pipeline_groups_GridSearchCV_SVC(X,y,group,i*42,2)\n",
    "#     print(grid.best_params_)\n",
    "#     print('best CV score:',grid.best_score_)\n",
    "#     print('test score:',test_score)\n",
    "#     test_scores_SVC.append(test_score)\n",
    "# print('test accuracy:',np.around(np.mean(test_scores_SVC),2),'+/-',np.around(np.std(test_scores_SVC),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "def ML_pipeline_groups_GridSearchCV_Logistic(X,y,groups,random_state,n_folds):\n",
    "    # create a test set based on groups\n",
    "    splitter = GroupShuffleSplit(n_splits=1,test_size=0.2,random_state=random_state)\n",
    "    for i_other,i_test in splitter.split(X, y, groups):\n",
    "        X_other, y_other, groups_other = X.iloc[i_other], y[i_other], groups[i_other]\n",
    "        X_test, y_test, groups_test = X.iloc[i_test], y[i_test], groups[i_test]\n",
    "    # splitter for _other\n",
    "    kf = GroupKFold(n_splits=n_folds)\n",
    "    # create the pipeline: preprocessor + supervised ML method\n",
    "    cat_ftrs = ['channel_name']\n",
    "    cont_ftrs = ['alpha', 'theta', 'slowwave', 'sigma']\n",
    "    cat_transformer = Pipeline(steps = [\n",
    "        ('imputer1', SimpleImputer(missing_values='0.0', strategy='constant',fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(sparse = False, categories = 'auto'))])\n",
    "    cont_transformer = Pipeline(steps = [\n",
    "        ('imputer2', SimpleImputer(missing_values = np.nan,strategy = 'mean')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    preprocessor = ColumnTransformer(remainder='passthrough',\n",
    "    transformers=[\n",
    "        ('num', cont_transformer, cont_ftrs),\n",
    "        ('cat', cat_transformer, cat_ftrs)])\n",
    "    \n",
    "    # make overall pipeline\n",
    "    pipe = make_pipeline(\n",
    "        preprocessor,\n",
    "        LogisticRegression(penalty = 'l1', solver = 'saga', max_iter = 1000, multi_class = 'multinomial'))\n",
    "\n",
    "    # the parameter(s) we want to tune\n",
    "    param_grid = {'logisticregression__C': np.logspace(-2,2,num=5)}\n",
    "    # prepare gridsearch\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid,scoring = make_scorer(accuracy_score),\n",
    "                        cv=kf, return_train_score = True,iid=True, n_jobs = -1)\n",
    "    # do kfold CV on _other\n",
    "    grid.fit(X_other, y_other, groups_other)\n",
    "    return grid, grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 0 process\n",
      "{'logisticregression__C': 1.0}\n",
      "best CV score: 0.6240503012837306\n",
      "test score: 0.7443181818181818\n",
      "Starting: 1 process\n",
      "{'logisticregression__C': 0.01}\n",
      "best CV score: 0.6500014695940981\n",
      "test score: 0.5604308985811876\n",
      "Starting: 2 process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 10.0}\n",
      "best CV score: 0.6702455337498902\n",
      "test score: 0.6205954897815363\n",
      "Starting: 3 process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 100.0}\n",
      "best CV score: 0.6689908796704913\n",
      "test score: 0.5892061828661252\n",
      "Starting: 4 process\n",
      "{'logisticregression__C': 100.0}\n",
      "best CV score: 0.6308379625547423\n",
      "test score: 0.6586967945349448\n",
      "test accuracy: 0.63 +/- 0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "test_scores_logistic = []\n",
    "for i in range(5):\n",
    "    print('Starting:', i, 'process')\n",
    "    grid, test_score = ML_pipeline_groups_GridSearchCV_Logistic(X,y,group,i*42,2)\n",
    "    print(grid.best_params_)\n",
    "    print('best CV score:',grid.best_score_)\n",
    "    print('test score:',test_score)\n",
    "    test_scores_logistic.append(test_score)\n",
    "print('test accuracy:',np.around(np.mean(test_scores_logistic),2),'+/-',np.around(np.std(test_scores_logistic),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "def ML_pipeline_groups_GridSearchCV_RandomForest(X,y,groups,random_state,n_folds):\n",
    "    # create a test set based on groups\n",
    "    splitter = GroupShuffleSplit(n_splits=1,test_size=0.2,random_state=random_state)\n",
    "    for i_other,i_test in splitter.split(X, y, groups):\n",
    "        X_other, y_other, groups_other = X.iloc[i_other], y[i_other], groups[i_other]\n",
    "        X_test, y_test, groups_test = X.iloc[i_test], y[i_test], groups[i_test]\n",
    "    # splitter for _other\n",
    "    kf = GroupKFold(n_splits=n_folds)\n",
    "    # create the pipeline: preprocessor + supervised ML method\n",
    "    cat_ftrs = ['channel_name']\n",
    "    cont_ftrs = ['alpha', 'theta', 'slowwave', 'sigma']\n",
    "    cat_transformer = Pipeline(steps = [\n",
    "        ('imputer1', SimpleImputer(missing_values='0.0', strategy='constant',fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(sparse = False, categories = 'auto'))])\n",
    "    cont_transformer = Pipeline(steps = [\n",
    "        ('imputer2', SimpleImputer(missing_values = np.nan,strategy = 'mean')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    preprocessor = ColumnTransformer(remainder='passthrough',\n",
    "    transformers=[\n",
    "        ('num', cont_transformer, cont_ftrs),\n",
    "        ('cat', cat_transformer, cat_ftrs)])\n",
    "    \n",
    "    # make overall pipeline\n",
    "    pipe = make_pipeline(\n",
    "        preprocessor,\n",
    "        RandomForestClassifier(random_state= random_state))\n",
    "    \n",
    "    # specify parameters \n",
    "    param_grid = {'randomforestclassifier__max_depth' : np.logspace(0, 3, num=5), \n",
    "                  'randomforestclassifier__n_estimators' : np.linspace(1, 100, num = 5, dtype = int)}\n",
    "    # prepare gridsearch\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid,scoring = make_scorer(accuracy_score),\n",
    "                        cv=kf, return_train_score = True,iid=True, n_jobs = -1)\n",
    "    # do kfold CV on _other\n",
    "    grid.fit(X_other, y_other, groups_other)\n",
    "    return grid, grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 0 process\n",
      "{'randomforestclassifier__max_depth': 31.622776601683793, 'randomforestclassifier__n_estimators': 100}\n",
      "best CV score: 0.6520536779902775\n",
      "test score: 0.738546176046176\n",
      "Starting: 1 process\n",
      "{'randomforestclassifier__max_depth': 31.622776601683793, 'randomforestclassifier__n_estimators': 75}\n",
      "best CV score: 0.6450048496605237\n",
      "test score: 0.6138553161674549\n",
      "Starting: 2 process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'randomforestclassifier__max_depth': 177.82794100389228, 'randomforestclassifier__n_estimators': 75}\n",
      "best CV score: 0.6883158790225586\n",
      "test score: 0.6526603241719521\n",
      "Starting: 3 process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'randomforestclassifier__max_depth': 31.622776601683793, 'randomforestclassifier__n_estimators': 100}\n",
      "best CV score: 0.671726978523095\n",
      "test score: 0.643611911623439\n",
      "Starting: 4 process\n",
      "{'randomforestclassifier__max_depth': 31.622776601683793, 'randomforestclassifier__n_estimators': 50}\n",
      "best CV score: 0.6694295035711136\n",
      "test score: 0.6779646172709757\n",
      "test accuracy: 0.67 +/- 0.04\n"
     ]
    }
   ],
   "source": [
    "test_scores_randomForest = []\n",
    "for i in range(5):\n",
    "    print('Starting:', i, 'process')\n",
    "    grid, test_score = ML_pipeline_groups_GridSearchCV_RandomForest(X,y,group,i*42,2)\n",
    "    print(grid.best_params_)\n",
    "    print('best CV score:',grid.best_score_)\n",
    "    print('test score:',test_score)\n",
    "    test_scores_randomForest.append(test_score)\n",
    "print('test accuracy:',np.around(np.mean(test_scores_randomForest),2),'+/-',np.around(np.std(test_scores_randomForest),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from xgboost import XGBClassifier\n",
    "def ML_pipeline_groups_GridSearchCV_XGboost(X,y,groups,random_state,n_folds):\n",
    "    # create a test set based on groups\n",
    "    splitter = GroupShuffleSplit(n_splits=1,test_size=0.2,random_state=random_state)\n",
    "    for i_other,i_test in splitter.split(X, y, groups):\n",
    "        X_other, y_other, groups_other = X.iloc[i_other], y[i_other], groups[i_other]\n",
    "        X_test, y_test, groups_test = X.iloc[i_test], y[i_test], groups[i_test]\n",
    "    # splitter for _other\n",
    "    kf = GroupKFold(n_splits=n_folds)\n",
    "    # create the pipeline: preprocessor + supervised ML method\n",
    "    cat_ftrs = ['channel_name']\n",
    "    cont_ftrs = ['alpha', 'theta', 'slowwave', 'sigma']\n",
    "    cat_transformer = Pipeline(steps = [\n",
    "        ('imputer1', SimpleImputer(missing_values='0.0', strategy='constant',fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(sparse = False, categories = 'auto'))])\n",
    "    cont_transformer = Pipeline(steps = [\n",
    "        ('scaler', StandardScaler())])\n",
    "    preprocessor = ColumnTransformer(remainder='passthrough',\n",
    "    transformers=[\n",
    "        ('num', cont_transformer, cont_ftrs),\n",
    "        ('cat', cat_transformer, cat_ftrs)])\n",
    "    \n",
    "    # make overall pipeline\n",
    "    pipe = make_pipeline(\n",
    "        preprocessor,\n",
    "        XGBClassifier(seed = random_state))\n",
    "    \n",
    "    # specify parameters \n",
    "    param_grid = {\"xgbclassifier__reg_alpha\":np.logspace(-2,2,num=5) }\n",
    "    # prepare gridsearch\n",
    "    grid = GridSearchCV(pipe, param_grid=(param_grid),scoring = make_scorer(accuracy_score),\n",
    "                        cv=kf, return_train_score = True,iid=True, n_jobs = -1)\n",
    "    # do kfold CV on _other\n",
    "    grid.fit(X_other, y_other, groups_other)\n",
    "    return grid, grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 0 process\n",
      "{'xgbclassifier__reg_alpha': 1.0}\n",
      "best CV score: 0.6285622798591098\n",
      "test score: 0.7369227994227994\n",
      "Starting: 1 process\n",
      "{'xgbclassifier__reg_alpha': 0.01}\n",
      "best CV score: 0.6525291714428475\n",
      "test score: 0.5638465580662112\n",
      "Starting: 2 process\n",
      "{'xgbclassifier__reg_alpha': 0.1}\n",
      "best CV score: 0.6726510017894335\n",
      "test score: 0.6375088090204369\n",
      "Starting: 3 process\n",
      "{'xgbclassifier__reg_alpha': 1.0}\n",
      "best CV score: 0.6698440717858194\n",
      "test score: 0.5871102960440137\n",
      "Starting: 4 process\n",
      "{'xgbclassifier__reg_alpha': 0.1}\n",
      "best CV score: 0.6447109308409017\n",
      "test score: 0.6680679628656507\n",
      "test accuracy: 0.64 +/- 0.06\n"
     ]
    }
   ],
   "source": [
    "test_scores_xgboost = []\n",
    "for i in range(5):\n",
    "    print('Starting:', i, 'process')\n",
    "    grid, test_score = ML_pipeline_groups_GridSearchCV_XGboost(X,y,group,i*42,2)\n",
    "    print(grid.best_params_)\n",
    "    print('best CV score:',grid.best_score_)\n",
    "    print('test score:',test_score)\n",
    "    test_scores_xgboost.append(test_score)\n",
    "print('test accuracy:',np.around(np.mean(test_scores_xgboost),2),'+/-',np.around(np.std(test_scores_xgboost),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: 0 process\n",
      "{'xgbclassifier__reg_alpha': 0.01}\n",
      "best CV score: 0.6231770151078508\n",
      "test score: 0.73502886002886\n",
      "Starting: 1 process\n",
      "{'xgbclassifier__reg_alpha': 0.01}\n",
      "best CV score: 0.6798930135496576\n",
      "test score: 0.5638465580662112\n",
      "Starting: 2 process\n",
      "{'xgbclassifier__reg_alpha': 0.01}\n",
      "best CV score: 0.6609463463287277\n",
      "test score: 0.6373326286116984\n",
      "Starting: 3 process\n",
      "{'xgbclassifier__reg_alpha': 0.1}\n",
      "best CV score: 0.6652544866137099\n",
      "test score: 0.5896428259540651\n",
      "Starting: 4 process\n",
      "{'xgbclassifier__reg_alpha': 0.01}\n",
      "best CV score: 0.6492960644270053\n",
      "test score: 0.6692065160273253\n",
      "test accuracy: 0.64 +/- 0.06\n"
     ]
    }
   ],
   "source": [
    "test_scores_xgboost_2 = []\n",
    "for i in range(5):\n",
    "    print('Starting:', i, 'process')\n",
    "    grid, test_score = ML_pipeline_groups_GridSearchCV_XGboost(X,y,group,i*42,5)\n",
    "    print(grid.best_params_)\n",
    "    print('best CV score:',grid.best_score_)\n",
    "    print('test score:',test_score)\n",
    "    test_scores_xgboost_2.append(test_score)\n",
    "print('test accuracy:',np.around(np.mean(test_scores_xgboost_2),2),'+/-',np.around(np.std(test_scores_xgboost_2),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
